---
- name: Initialize Kubernetes control plane (first node)
  hosts: control_plane
  become: true
  tasks:
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_initialized

    - name: Initialize Kubernetes cluster (first control plane node)
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_cidr }} \
          --apiserver-advertise-address={{ k8s_host }} \
          --control-plane-endpoint="{{ groups['control_plane'][0] }}.local:6443" \
          --upload-certs
      when: not kubeadm_initialized.stat.exists and inventory_hostname == groups['control_plane'][0]
      register: kubeadm_init
      failed_when: kubeadm_init.rc != 0

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        mode: 0755
      when: inventory_hostname == groups['control_plane'][0]

    - name: Copy kubeconfig for root
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        owner: root
        group: root
        mode: 0600
      when: inventory_hostname == groups['control_plane'][0]

    - name: Create .kube directory for user
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0755
      when: inventory_hostname == groups['control_plane'][0]

    - name: Copy kubeconfig for user
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: true
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0600
      when: inventory_hostname == groups['control_plane'][0]

    - name: Wait for kube-apiserver to be ready
      shell: kubectl get nodes
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      register: api_check
      until: api_check.rc == 0
      retries: 30
      delay: 2
      when: inventory_hostname == groups['control_plane'][0]

    - name: Ensure kubelet is running on first node
      systemd:
        name: kubelet
        state: started
        enabled: true
      when: inventory_hostname == groups['control_plane'][0]

    - name: Install Flannel CNI
      shell: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      when: inventory_hostname == groups['control_plane'][0]
      register: flannel_install

    - name: Wait for Flannel pods to be ready
      shell: kubectl wait --for=condition=Ready pod -l app=flannel -n kube-flannel --timeout=300s
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      when: inventory_hostname == groups['control_plane'][0]
      retries: 3
      delay: 10
      register: flannel_ready
      until: flannel_ready.rc == 0

    - name: Create etcd snapshot backup
      shell: |
        export ETCDCTL_API=3
        etcdctl \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          snapshot save /root/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db
      when: inventory_hostname == groups['control_plane'][0]
      ignore_errors: true


- name: Join additional control plane nodes
  hosts: control_plane
  become: true
  tasks:
    - name: Check if node is already in cluster
      shell: kubectl get nodes --kubeconfig=/etc/kubernetes/admin.conf 2>/dev/null | grep -q "{{ inventory_hostname }}"
      register: node_in_cluster
      failed_when: false
      changed_when: false
      when: inventory_hostname != groups['control_plane'][0]

    - name: Generate join command for additional control plane nodes
      shell: kubeadm token create --print-join-command --ttl 2h
      register: join_command_output_cp
      when: inventory_hostname == groups['control_plane'][0]

    - name: Upload certificates for additional control plane nodes
      shell: kubeadm init phase upload-certs --upload-certs
      register: upload_certs_output
      when: inventory_hostname == groups['control_plane'][0]

    - name: Share join data with all hosts
      set_fact:
        join_command_cp: "{{ join_command_output_cp.stdout }}"
        certificate_key_cp: "{{ upload_certs_output.stdout_lines[-1] }}"
      when: inventory_hostname == groups['control_plane'][0]

    - name: Join additional control plane nodes to cluster
      shell: |
        {{ hostvars[groups['control_plane'][0]].join_command_cp }} \
        --control-plane \
        --certificate-key {{ hostvars[groups['control_plane'][0]].certificate_key_cp }} \
        --apiserver-advertise-address={{ k8s_host }}
      when: inventory_hostname != groups['control_plane'][0] and node_in_cluster.rc != 0
      register: join_result
      delay: 600
      retries: 0
      until: join_result.rc == 0

    - name: Ensure kubelet is running on joined nodes
      systemd:
        name: kubelet
        state: restarted
        enabled: true
      when: inventory_hostname != groups['control_plane'][0]

    - name: Create .kube directory for root on additional nodes
      file:
        path: /root/.kube
        state: directory
        mode: 0755
      when: inventory_hostname != groups['control_plane'][0]

    - name: Copy kubeconfig for root on additional nodes
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        owner: root
        group: root
        mode: 0600
      when: inventory_hostname != groups['control_plane'][0]

    - name: Create .kube directory for user on additional nodes
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0755
      when: inventory_hostname != groups['control_plane'][0]

    - name: Copy kubeconfig for user on additional nodes
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: true
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0600
      when: inventory_hostname != groups['control_plane'][0]

    - name: Clean up join command
      file:
        path: /tmp/kubeadm_join_cp_command.sh
        state: absent
      when: inventory_hostname != groups['control_plane'][0]

    - name: Clean up join command on first node
      file:
        path: /tmp/kubeadm_join_cp_command.sh
        state: absent
      when: inventory_hostname == groups['control_plane'][0]
      run_once: true

- name: Remove control plane taints to allow workload scheduling
  hosts: control_plane
  tasks:
    - name: Wait for all nodes to be registered
      shell: kubectl get nodes --no-headers | wc -l
      register: node_count
      until: node_count.stdout | int == groups['control_plane'] | length
      retries: 30
      delay: 2
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      run_once: true

    - name: Remove NoSchedule taint from control plane nodes
      shell: |
        kubectl taint nodes {{ inventory_hostname }} node-role.kubernetes.io/control-plane:NoSchedule- || true
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      delegate_to: "{{ groups['control_plane'][0] }}"
      run_once: false

- name: Validate cluster health
  hosts: control_plane
  tasks:
    - name: Check all nodes are Ready
      shell: kubectl get nodes -o wide
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      register: cluster_nodes
      run_once: true

    - name: Display cluster status
      debug:
        var: cluster_nodes.stdout_lines
      run_once: true

    - name: Verify etcd cluster health
      shell: |
        ETCDCTL_API=3 etcdctl \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          endpoint health 2>&1
      become: true
      register: etcd_health
      run_once: true
      ignore_errors: true
      retries: 3
      delay: 5
      until: etcd_health.rc == 0

    - name: Display etcd health
      debug:
        msg: "{{ etcd_health.stdout }}"
      run_once: true
      when: etcd_health.rc == 0

- name: Fetch kubeconfig to local machine
  hosts: control_plane
  tasks:
    - name: Fetch kubeconfig
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ playbook_dir }}/../kubeconfig"
        flat: true
      become: true
      run_once: true